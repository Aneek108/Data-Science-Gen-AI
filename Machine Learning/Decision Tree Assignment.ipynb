{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fd7c06",
   "metadata": {},
   "source": [
    "# Decision Tree Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7bd80",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563bdbbe",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a popular supervised machine learning algorithm used for both classification and regression tasks. In the context of classification, it helps you decide the class (category) of a given input by learning simple decision rules inferred from the data features.\n",
    "\n",
    "### How does a Decision Tree work for classification?\n",
    "\n",
    "Think of a decision tree like a flowchart where:\n",
    "\n",
    "* **Nodes** represent tests on features (e.g., is \"Age > 30?\").\n",
    "* **Branches** represent the outcome of those tests (yes or no).\n",
    "* **Leaves** represent the final decision or class label (e.g., \"Will buy product\" or \"Won't buy product\").\n",
    "\n",
    "#### Step-by-step process:\n",
    "\n",
    "1. **Start at the root node**, which considers the entire dataset.\n",
    "2. **Choose the best feature** to split the data based on some criteria (like Information Gain or Gini Impurity), aiming to separate the data into distinct classes.\n",
    "3. **Split the dataset** into subsets according to the chosen feature’s values.\n",
    "4. **Repeat the process recursively** on each subset (child nodes), selecting features and splitting until:\n",
    "\n",
    "   * All samples in a node belong to the same class, or\n",
    "   * No more features are available, or\n",
    "   * A stopping criterion like max depth or minimum samples per node is reached.\n",
    "5. **Assign class labels** to the leaves based on the majority class in those nodes.\n",
    "\n",
    "### Example:\n",
    "\n",
    "If you have a dataset predicting whether someone will play tennis based on weather conditions, a decision tree might ask:\n",
    "\n",
    "* Is it sunny?\n",
    "\n",
    "  * Yes: Is humidity high?\n",
    "\n",
    "    * Yes → Don’t play tennis\n",
    "    * No → Play tennis\n",
    "  * No: Play tennis\n",
    "\n",
    "### Why Decision Trees?\n",
    "\n",
    "* Easy to understand and interpret.\n",
    "* Requires little data preprocessing.\n",
    "* Can handle both numerical and categorical data.\n",
    "* Non-linear relationships handled naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83fe23",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b27142",
   "metadata": {},
   "source": [
    "### What is an impurity measure?\n",
    "\n",
    "* It quantifies how *impure* or *uncertain* the class distribution is within a node.\n",
    "* The goal of a split in a decision tree is to reduce impurity, ideally resulting in child nodes that are as *pure* as possible (i.e., containing mostly one class).\n",
    "\n",
    "### 1. Gini Impurity\n",
    "\n",
    "Gini Impurity measures the probability of misclassifying a randomly chosen element if it was labeled according to the distribution of labels in the node.\n",
    "\n",
    "$$\n",
    "\\text{Gini} = 1 - \\sum_{i=1}^C p_i^2\n",
    "$$\n",
    "\n",
    "* $p_i$ = proportion of class $i$ in the node.\n",
    "* $C$ = number of classes.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* Gini impurity is 0 when all samples belong to one class (pure node).\n",
    "* It’s maximum when classes are equally mixed.\n",
    "\n",
    "### 2. Entropy\n",
    "\n",
    "Entropy comes from information theory and measures the amount of uncertainty or disorder in the node.\n",
    "\n",
    "$$\n",
    "\\text{Entropy} = - \\sum_{i=1}^C p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "* $p_i$ same as above.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* Entropy is 0 if the node is pure.\n",
    "* It’s maximum when the classes are equally mixed.\n",
    "\n",
    "### How do they impact splits?\n",
    "\n",
    "* During tree building, the algorithm looks for splits that **reduce impurity the most**.\n",
    "* The *reduction* is called **Information Gain** (using Entropy) or **Gini Gain** (using Gini Impurity).\n",
    "\n",
    "**Formula for Information Gain (Entropy):**\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Entropy(before split)} - \\sum_{k} \\frac{N_k}{N} \\times \\text{Entropy(after split}_k)\n",
    "$$\n",
    "\n",
    "Similarly for Gini Gain, replacing Entropy with Gini.\n",
    "\n",
    "### Choosing splits:\n",
    "\n",
    "* The algorithm evaluates all possible splits for all features.\n",
    "* For each split, it calculates impurity for the child nodes.\n",
    "* It picks the split that results in the **highest impurity reduction**.\n",
    "\n",
    "### Differences and practical notes:\n",
    "\n",
    "* Gini Impurity is faster to compute and often preferred in implementations like **scikit-learn**.\n",
    "* Entropy is more theoretically grounded in information theory.\n",
    "* Usually, both lead to very similar trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ead5ce",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e62233",
   "metadata": {},
   "source": [
    "When a decision tree grows fully, it might fit the training data too closely, capturing noise instead of the underlying pattern — this is **overfitting**. Pruning helps reduce that by cutting back the tree.\n",
    "\n",
    "### a. Pre-Pruning (Early Stopping)\n",
    "\n",
    "* **What it is:** You stop the tree from growing too deep **during** the training process.\n",
    "* You set conditions (like max depth, minimum samples per leaf, or minimum impurity decrease) to halt splitting early.\n",
    "* The tree is prevented from becoming too complex.\n",
    "\n",
    "#### Practical advantage of Pre-Pruning:\n",
    "\n",
    "* **Faster training time** because the tree is smaller and simpler from the start.\n",
    "* It can save computation and prevent overfitting early on without needing extra steps.\n",
    "\n",
    "### b. Post-Pruning (Prune after full growth)\n",
    "\n",
    "* **What it is:** First, grow a **full tree** (potentially overfitting).\n",
    "* Then, **cut back** or remove branches that don’t improve performance on a validation set (or by some cost-complexity measure).\n",
    "* This typically involves evaluating subtree removal to see if it improves generalization.\n",
    "\n",
    "#### Practical advantage of Post-Pruning:\n",
    "\n",
    "* **Potentially better accuracy** because you explore the full complexity before simplifying.\n",
    "* It can find a better balance by removing only the truly unnecessary branches.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Aspect        | Pre-Pruning                     | Post-Pruning                        |\n",
    "| ------------- | ------------------------------- | ----------------------------------- |\n",
    "| When pruning? | During tree growth              | After full tree is grown            |\n",
    "| Advantage     | Faster training, simple model   | Often more accurate and flexible    |\n",
    "| Risk          | Might stop too early (underfit) | Needs extra validation step, slower |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2bd68",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. What is Information Gain in Decision Trees, and why is it important for choosing the best split? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e06531",
   "metadata": {},
   "source": [
    "**Information Gain (IG)** measures how much *uncertainty* (or impurity) is reduced after splitting a dataset based on a particular feature.\n",
    "\n",
    "* It’s based on the concept of **Entropy** from information theory.\n",
    "* The idea: a good split should separate data into groups that are as pure (homogeneous) as possible.\n",
    "\n",
    "### How is Information Gain calculated?\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = \\text{Entropy(before split)} - \\sum_{k=1}^{m} \\frac{N_k}{N} \\times \\text{Entropy(after split}_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $N$ = total number of samples before split.\n",
    "* $m$ = number of child nodes (branches) created by the split.\n",
    "* $N_k$ = number of samples in child node $k$.\n",
    "* $\\text{Entropy(before split)}$ = entropy of the parent node.\n",
    "* $\\text{Entropy(after split}_k)$ = entropy of child node $k$.\n",
    "\n",
    "### Why is Information Gain important?\n",
    "\n",
    "* **Choosing the best split:** The decision tree algorithm evaluates all possible splits on all features and selects the one with the **highest Information Gain**.\n",
    "* A split with high IG means the data is divided into groups with lower entropy, i.e., the classes are more clearly separated.\n",
    "* This helps the tree become more accurate and efficient by making better decisions at each node.\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "* Before splitting, you have uncertainty about the class labels.\n",
    "* After splitting, if the uncertainty is reduced significantly, the split is valuable.\n",
    "* Information Gain quantifies that reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0750f",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236cdf35",
   "metadata": {},
   "source": [
    "### Common Real-World Applications of Decision Trees\n",
    "\n",
    "1. **Healthcare**\n",
    "\n",
    "   * Diagnosing diseases based on symptoms and patient data.\n",
    "   * Predicting patient outcomes or risk stratification.\n",
    "\n",
    "2. **Finance**\n",
    "\n",
    "   * Credit scoring and risk assessment.\n",
    "   * Fraud detection by classifying transactions.\n",
    "\n",
    "3. **Marketing**\n",
    "\n",
    "   * Customer segmentation.\n",
    "   * Predicting customer churn or purchase behavior.\n",
    "\n",
    "4. **Manufacturing**\n",
    "\n",
    "   * Quality control by classifying defective vs. non-defective products.\n",
    "   * Predictive maintenance based on sensor data.\n",
    "\n",
    "5. **Customer Support**\n",
    "\n",
    "   * Automating decision-making in chatbots.\n",
    "   * Routing customer queries based on issue type.\n",
    "   \n",
    "### Advantages of Decision Trees\n",
    "\n",
    "* **Easy to interpret and visualize:** Decision rules are straightforward and understandable.\n",
    "* **Handles both numerical and categorical data:** No need for complex preprocessing.\n",
    "* **Non-parametric:** No assumptions about the data distribution.\n",
    "* **Handles multi-class classification and regression tasks.**\n",
    "* **Works well with missing data:** Can handle missing values during training.\n",
    "\n",
    "### Limitations of Decision Trees\n",
    "\n",
    "* **Overfitting:** Trees can grow very deep and fit noise in training data.\n",
    "* **Instability:** Small changes in data can lead to very different trees.\n",
    "* **Bias towards features with more levels:** Features with many categories can dominate splits.\n",
    "* **Less accurate compared to ensemble methods:** Alone, they might underperform compared to random forests or gradient boosting.\n",
    "* **Greedy splitting:** The algorithm makes locally optimal splits, which might not lead to the best overall tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f5b59",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Write a Python program to: \n",
    "> * #### Load the Iris Dataset \n",
    "> * #### Train a Decision Tree Classifier using the Gini criterion \n",
    "> * #### Print the model’s accuracy and feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654c2aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "sepal length (cm): 0.000\n",
      "sepal width (cm): 0.019\n",
      "petal length (cm): 0.893\n",
      "petal width (cm): 0.088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree Classifier with Gini criterion\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "for name, importance in zip(iris.feature_names, feature_importances):\n",
    "    print(f\"{name}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f69e94",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Write a Python program to: \n",
    "> * #### Load the Iris Dataset \n",
    "> * #### Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa210503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with max_depth=3: 1.00\n",
      "Accuracy of fully grown Decision Tree: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Decision Tree with max_depth=3 (pruned tree)\n",
    "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "y_pred_pruned = clf_pruned.predict(X_test)\n",
    "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
    "\n",
    "# Fully grown Decision Tree (no max_depth)\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_pruned:.2f}\")\n",
    "print(f\"Accuracy of fully grown Decision Tree: {accuracy_full:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3630b23b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Write a Python program to: \n",
    "> * #### Load the Boston Housing Dataset \n",
    "> * #### Train a Decision Tree Regressor \n",
    "> * #### Print the Mean Squared Error (MSE) and feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82f3397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 10.4161\n",
      "\n",
      "Feature Importances:\n",
      "    Feature  Importance\n",
      "5        RM    0.600326\n",
      "12    LSTAT    0.193328\n",
      "7       DIS    0.070688\n",
      "0      CRIM    0.051296\n",
      "4       NOX    0.027148\n",
      "6       AGE    0.013617\n",
      "9       TAX    0.012464\n",
      "10  PTRATIO    0.011012\n",
      "11        B    0.009009\n",
      "2     INDUS    0.005816\n",
      "1        ZN    0.003353\n",
      "8       RAD    0.001941\n",
      "3      CHAS    0.000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Boston Housing dataset from OpenML\n",
    "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "\n",
    "# Extract features and target\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Decision Tree Regressor\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Print the MSE\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Combine feature names and their importances into a DataFrame for better readability\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01a0e1",
   "metadata": {},
   "source": [
    "---\n",
    "## 9.  Write a Python program to: \n",
    "> * #### Load the Iris Dataset \n",
    "> * #### Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV \n",
    "> * #### Print the best parameters and the resulting model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b780b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
      "Accuracy of best model: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on test set using best estimator\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of best model: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63652885",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: \n",
    "> * #### Handle the missing values \n",
    "> * #### Encode the categorical features \n",
    "> * #### Train a Decision Tree model \n",
    "> * #### Tune its hyperparameters \n",
    "> * #### Evaluate its performance And describe what business value this model could provide in the real-world setting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27314beb",
   "metadata": {},
   "source": [
    "### 1. **Handling Missing Values**\n",
    "\n",
    "* **Analyze missingness:** Check which features have missing data and the percentage missing.\n",
    "* **Decide on imputation strategy:**\n",
    "\n",
    "  * For **numerical features**: use mean, median, or more advanced methods like KNN imputation.\n",
    "  * For **categorical features**: use the most frequent category or create a separate category like \"Missing\".\n",
    "* **Apply imputation:** Use tools like `SimpleImputer` or `IterativeImputer` from scikit-learn to fill missing values consistently.\n",
    "* **Optional:** Consider if missingness itself is informative and create indicator features if needed.\n",
    "\n",
    "### 2. **Encoding Categorical Features**\n",
    "\n",
    "* Identify categorical features.\n",
    "* For **ordinal categories** (with a clear order): use **Ordinal Encoding**.\n",
    "* For **nominal categories** (no order): use **One-Hot Encoding** to create binary features.\n",
    "* If categories have high cardinality, consider techniques like **Target Encoding** or **Frequency Encoding**.\n",
    "* Use `ColumnTransformer` in scikit-learn to apply different transformations to numerical and categorical features seamlessly.\n",
    "\n",
    "### 3. **Training a Decision Tree Model**\n",
    "\n",
    "* Split data into **training** and **testing** sets to evaluate performance fairly.\n",
    "* Initialize a **Decision Tree Classifier**, e.g., `DecisionTreeClassifier` in scikit-learn.\n",
    "* Fit the model on the **training data** after preprocessing.\n",
    "* Decision Trees can handle mixed data types and do not require feature scaling.\n",
    "\n",
    "### 4. **Tuning Hyperparameters**\n",
    "\n",
    "* Use **GridSearchCV** or **RandomizedSearchCV** to tune important hyperparameters like:\n",
    "\n",
    "  * `max_depth`: to control tree complexity.\n",
    "  * `min_samples_split` and `min_samples_leaf`: to avoid overfitting on small data.\n",
    "  * `criterion`: 'gini' or 'entropy'.\n",
    "* Use cross-validation during tuning to ensure the model generalizes well.\n",
    "* Select the best hyperparameters based on validation performance (e.g., accuracy, recall, or AUC).\n",
    "\n",
    "### 5. **Evaluating Performance**\n",
    "\n",
    "* Evaluate the final model on the **test set**.\n",
    "* Use metrics relevant for healthcare and classification:\n",
    "\n",
    "  * **Accuracy** (overall correctness).\n",
    "  * **Precision and Recall**: especially important to minimize false negatives (missing a disease).\n",
    "  * **F1 Score**: balance between precision and recall.\n",
    "  * **ROC-AUC**: how well the model discriminates between classes.\n",
    "* Consider **confusion matrix** analysis to understand types of errors.\n",
    "\n",
    "### 6. **Business Value in Real-World Setting**\n",
    "\n",
    "* **Early and accurate disease detection:** Enables timely intervention and treatment, improving patient outcomes.\n",
    "* **Resource optimization:** Prioritize patients who need urgent care or further testing, saving costs.\n",
    "* **Personalized care:** Tailor treatment plans based on risk prediction.\n",
    "* **Reducing hospital readmissions:** Proactively monitor high-risk patients.\n",
    "* **Supporting doctors’ decision-making:** Provide data-driven insights that augment clinical judgment.\n",
    "* **Regulatory compliance and reporting:** Automated, consistent predictions can support audits and reporting requirements.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "| Step                        | Description                                     | Tools/Techniques                          |\n",
    "| --------------------------- | ----------------------------------------------- | ----------------------------------------- |\n",
    "| Handle Missing Values       | Impute with mean/median or create indicators    | `SimpleImputer`, `IterativeImputer`       |\n",
    "| Encode Categorical Features | One-hot, ordinal, target encoding               | `OneHotEncoder`, `OrdinalEncoder`         |\n",
    "| Train Decision Tree         | Fit on processed data                           | `DecisionTreeClassifier`                  |\n",
    "| Tune Hyperparameters        | Grid/random search with cross-validation        | `GridSearchCV`, `RandomizedSearchCV`      |\n",
    "| Evaluate Performance        | Use metrics like precision, recall, ROC-AUC     | `accuracy_score`, `classification_report` |\n",
    "| Business Value              | Early detection, cost-saving, personalized care | Better patient outcomes, efficiency       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
