{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83cfd6c2",
   "metadata": {},
   "source": [
    "# Ensemble Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76637c",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94599311",
   "metadata": {},
   "source": [
    "**Ensemble learning** is a technique in machine learning where **multiple models (often called â€œweak learnersâ€) are combined** to produce a **stronger, more accurate, and more robust predictive model**.\n",
    "\n",
    "Instead of relying on a single model, ensemble methods aggregate the predictions of several models to reduce errors, variance, and bias.\n",
    "\n",
    "### **Key Idea Behind Ensemble Learning**\n",
    "\n",
    "The central idea is that:\n",
    "\n",
    "* **â€œA group of weak models can come together to form a strong model.â€**\n",
    "* Each model may capture different aspects or patterns in the data.\n",
    "* By combining them, their individual mistakes can cancel out, leading to improved overall performance.\n",
    "\n",
    "This works similar to the idea of **â€œwisdom of the crowdâ€** â€” multiple opinions, when aggregated, are often more reliable than a single opinion.\n",
    "\n",
    "### **Why it works**\n",
    "\n",
    "1. **Reduces Variance** â†’ Averaging predictions (e.g., Bagging) prevents overfitting.\n",
    "2. **Reduces Bias** â†’ Combining diverse models (e.g., Boosting) helps capture complex patterns.\n",
    "3. **Improves Generalization** â†’ More robust to noise and unseen data.\n",
    "\n",
    "### **Common Ensemble Methods**\n",
    "\n",
    "* **Bagging (Bootstrap Aggregating):** Uses multiple models trained on random subsets of data (e.g., Random Forest).\n",
    "* **Boosting:** Sequentially trains models, giving more weight to misclassified instances (e.g., AdaBoost, Gradient Boosting, XGBoost).\n",
    "* **Stacking:** Combines different models and uses another â€œmeta-modelâ€ to make final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55cdbe",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. What is the difference between Bagging and Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fddf6",
   "metadata": {},
   "source": [
    "| **Aspect**             | **Bagging (Bootstrap Aggregating)**                                                        | **Boosting**                                                                                               |\n",
    "| ---------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------- |\n",
    "| **Goal**               | Reduce **variance** (overfitting)                                                          | Reduce **bias** (underfitting)                                                                             |\n",
    "| **How it works**       | Trains models **independently** on random subsets of data (with replacement)               | Trains models **sequentially**, where each new model focuses on correcting the errors of the previous ones |\n",
    "| **Data Sampling**      | Uses **bootstrap samples** (random sampling with replacement)                              | Uses the **entire dataset**, but adjusts weights of misclassified samples to focus on hard cases           |\n",
    "| **Model Combination**  | Combines predictions by **majority voting (classification)** or **averaging (regression)** | Combines predictions by **weighted voting/weighted sum**                                                   |\n",
    "| **Bias & Variance**    | Lowers **variance**, bias remains about the same                                           | Lowers **bias**, variance may slightly increase                                                            |\n",
    "| **Example Algorithms** | Random Forest                                                                              | AdaBoost, Gradient Boosting, XGBoost, LightGBM                                                             |\n",
    "| **Parallelization**    | Models can be trained in **parallel** (since they are independent)                         | Models must be trained **sequentially** (each depends on previous)                                         |\n",
    "\n",
    "\n",
    "### **Quick Intuition**\n",
    "\n",
    "* **Bagging:** â€œMany independent models vote â†’ reduces overfitting.â€\n",
    "* **Boosting:** â€œModels learn from each otherâ€™s mistakes â†’ reduces underfitting.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dfaba",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd32ab",
   "metadata": {},
   "source": [
    "**Bootstrap sampling** is a statistical resampling technique where we create new datasets by **randomly selecting samples *with replacement*** from the original dataset.\n",
    "\n",
    "* If the original dataset has **N samples**, each bootstrap sample also has **N samples**.\n",
    "* Since selection is **with replacement**, some data points may appear multiple times, while others may not appear at all in a given bootstrap sample.\n",
    "\n",
    "### **Role in Bagging (e.g., Random Forest)**\n",
    "\n",
    "1. **Diversity in Models**\n",
    "\n",
    "   * Each model (e.g., each decision tree in a Random Forest) is trained on a different bootstrap sample.\n",
    "   * This ensures the models are not identical and capture different patterns.\n",
    "\n",
    "2. **Reduces Overfitting (Variance Reduction)**\n",
    "\n",
    "   * By averaging the predictions of multiple diverse models, Bagging smooths out noise and reduces variance.\n",
    "\n",
    "3. **Out-of-Bag (OOB) Error Estimation**\n",
    "\n",
    "   * Since \\~36% of the data is typically left out of each bootstrap sample, these â€œout-of-bagâ€ samples can be used to estimate model accuracy without needing a separate validation set.\n",
    "\n",
    "### **Example (Random Forest)**\n",
    "\n",
    "* Suppose you have a dataset of 1000 rows.\n",
    "* You draw 1000 samples *with replacement* â†’ this is one bootstrap sample.\n",
    "* Train **Tree 1** on this bootstrap sample.\n",
    "* Repeat the process to generate bootstrap samples for **Tree 2, Tree 3, â€¦**\n",
    "* Final prediction = **majority voting (classification)** or **averaging (regression)** across all trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4ab8a",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3dba7",
   "metadata": {},
   "source": [
    "### **Out-of-Bag (OOB) Samples**\n",
    "\n",
    "* In **bootstrap sampling**, each new dataset is created by sampling **with replacement** from the original dataset.\n",
    "* On average, about **63% of the data points** are included in a bootstrap sample, while the remaining **\\~37% are left out**.\n",
    "* These **left-out data points** for a given bootstrap sample are called **Out-of-Bag (OOB) samples**.\n",
    "\n",
    "### **How OOB Score is Used**\n",
    "\n",
    "1. **Training**\n",
    "\n",
    "   * Each base model (e.g., a decision tree in Random Forest) is trained on its bootstrap sample.\n",
    "\n",
    "2. **Evaluation with OOB samples**\n",
    "\n",
    "   * The OOB samples (the \\~37% not used in training for that model) are used as a **validation set** to test the modelâ€™s performance.\n",
    "   * Since every data point is likely to be OOB for some models, we can evaluate the performance of the entire ensemble using these unused samples.\n",
    "\n",
    "3. **OOB Score**\n",
    "\n",
    "   * The **OOB score** is the average accuracy (for classification) or RÂ² / error metric (for regression) calculated using OOB samples across all models.\n",
    "   * It provides a reliable, unbiased estimate of model performance **without needing a separate validation or test dataset**.\n",
    "\n",
    "### **Advantages of OOB Score**\n",
    "\n",
    "* Saves data â†’ No need to split into training/validation sets.\n",
    "* Gives a built-in performance check during model training.\n",
    "* Especially useful in Random Forests for quick evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac396b",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326b85e",
   "metadata": {},
   "source": [
    "| **Aspect**               | **Decision Tree**                                                                                                                          | **Random Forest**                                                                                                                     |\n",
    "| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **How it is calculated** | Based on how much each feature **reduces impurity** (e.g., Gini Index, Entropy for classification; Variance for regression) at its splits. | Computed by **averaging the feature importance scores** from all the decision trees in the forest.                                    |\n",
    "| **Bias**                 | Can be **biased toward features with more categories** (categorical variables with many levels may appear more important).                 | Bias is **reduced** since importance is averaged across many trees trained on different bootstrap samples and random feature subsets. |\n",
    "| **Stability**            | Can be **unstable** â†’ small changes in data may lead to very different importance values.                                                  | More **stable and reliable** due to aggregation across multiple trees.                                                                |\n",
    "| **Interpretability**     | Easy to understand and explain because it comes from a single treeâ€™s structure.                                                            | Harder to interpret directly, but more trustworthy for generalization.                                                                |\n",
    "| **Overfitting**          | Prone to overfitting, so feature importance may not generalize well.                                                                       | Less prone to overfitting, so feature importance is more representative.                                                              |\n",
    "\n",
    "### **Key Idea**\n",
    "\n",
    "* **Decision Tree** â†’ Feature importance = â€œHow much this feature helped reduce impurity in this one tree.â€\n",
    "* **Random Forest** â†’ Feature importance = â€œHow much this feature consistently helped reduce impurity across many trees trained on different samples and subsets.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f22ead",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Write a Python program to:\n",
    "> * #### Load the Breast Cancer dataset using `sklearn.datasets.load_breast_cancer()`\n",
    "> * #### Train a Random Forest Classifier\n",
    "> * #### Print the top 5 most important features based on feature importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233f8399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Important Features:\n",
      "                 Feature  Importance\n",
      "23            worst area    0.139357\n",
      "27  worst concave points    0.132225\n",
      "7    mean concave points    0.107046\n",
      "20          worst radius    0.082848\n",
      "22       worst perimeter    0.080850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importance scores\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feat_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance and print top 5\n",
    "top5 = feat_importances.sort_values(by=\"Importance\", ascending=False).head(5)\n",
    "print(\"Top 5 Important Features:\")\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdd50f",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Write a Python program to:\n",
    "> * #### Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "> * #### Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "561c8a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Single Decision Tree: 1.0\n",
      "Accuracy of Bagging Classifier : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train a single Decision Tree\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "y_pred_tree = dtree.predict(X_test)\n",
    "tree_acc = accuracy_score(y_test, y_pred_tree)\n",
    "\n",
    "# Train a Bagging Classifier with Decision Trees\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,        # number of trees\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bag = bagging.predict(X_test)\n",
    "bagging_acc = accuracy_score(y_test, y_pred_bag)\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy of Single Decision Tree:\", tree_acc)\n",
    "print(\"Accuracy of Bagging Classifier :\", bagging_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dd5ea4",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Write a Python program to:\n",
    "> * #### Train a Random Forest Classifier\n",
    "> * #### Tune hyperparameters `max_depth` and `n_estimators` using GridSearchCV\n",
    "> * #### Print the best parameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adecdb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
      "Final Accuracy on Test Set: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Define Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # number of trees\n",
    "    'max_depth': [None, 5, 10]       # maximum depth of trees\n",
    "}\n",
    "\n",
    "# Grid Search with Cross-Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,               # 5-fold cross-validation\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "final_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Final Accuracy on Test Set:\", final_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11995cf9",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Write a Python program to:\n",
    "> * #### Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
    "> * #### Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf893ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Bagging Regressor): 0.2578738225058504\n",
      "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train Bagging Regressor (with Decision Trees as base estimator)\n",
    "bagging_reg = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_reg.predict(X_test)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Errors\n",
    "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
    "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48c7f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
    "> * #### Choose between Bagging or Boosting\n",
    "> * #### Handle overfitting\n",
    "> * #### Select base models\n",
    "> * #### Evaluate performance using cross-validation\n",
    "> * #### Justify how ensemble learning improves decision-making in this real-world context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594f336",
   "metadata": {},
   "source": [
    "### **1. Choose between Bagging or Boosting**\n",
    "\n",
    "* **Bagging** (e.g., Random Forest) is useful when the model has **high variance** (like Decision Trees), as it reduces overfitting by averaging predictions.\n",
    "* **Boosting** (e.g., XGBoost, AdaBoost, LightGBM) is better when the dataset is **complex and prone to underfitting**, as it sequentially corrects errors and reduces bias.\n",
    "\n",
    "ðŸ‘‰ For loan default prediction, where patterns are complex and misclassification is costly, **Boosting** is often preferred because it gives higher accuracy and better handles difficult-to-predict cases.\n",
    "\n",
    "### **2. Handle Overfitting**\n",
    "\n",
    "* Use **regularization parameters** in boosting methods (e.g., `learning_rate`, `max_depth`, `min_child_weight` in XGBoost).\n",
    "* Apply **early stopping** during training to prevent models from memorizing noise.\n",
    "* Use **cross-validation** to tune hyperparameters and avoid overfitting to training data.\n",
    "\n",
    "### **3. Select Base Models**\n",
    "\n",
    "* For **Bagging**: Base model = **Decision Trees** (prone to variance, but bagging stabilizes them).\n",
    "* For **Boosting**: Base model = **Weak learners (shallow Decision Trees)**, typically depth 3â€“5.\n",
    "\n",
    "ðŸ‘‰ In practice: Start with **Decision Trees** as base learners, then try Gradient Boosted Trees (XGBoost/LightGBM).\n",
    "\n",
    "### **4. Evaluate Performance using Cross-Validation**\n",
    "\n",
    "* Use **Stratified k-Fold Cross-Validation** since this is a **classification problem with class imbalance** (loan default vs non-default).\n",
    "* Evaluate metrics:\n",
    "\n",
    "  * **Accuracy** (overall correctness)\n",
    "  * **Precision & Recall** (important to reduce false negatives â†’ missing defaults is costly)\n",
    "  * **ROC-AUC** (to capture ranking quality of predicted probabilities).\n",
    "\n",
    "### **5. Justify how Ensemble Learning improves Decision-Making**\n",
    "\n",
    "* **Higher accuracy**: Boosting reduces bias and improves predictive power.\n",
    "* **More reliable risk prediction**: Helps financial institutions **identify risky borrowers more accurately**.\n",
    "* **Balanced decision-making**: Reduces errors in both directions â€” avoids lending to high-risk customers while not unfairly rejecting safe ones.\n",
    "* **Business impact**: Minimizes financial losses, improves credit risk models, and builds trust with regulators and customers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
