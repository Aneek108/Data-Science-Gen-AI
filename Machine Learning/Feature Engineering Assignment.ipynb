{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650669ac",
   "metadata": {},
   "source": [
    "# Feature Engineering Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a8156",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f617ba8",
   "metadata": {},
   "source": [
    "In **Machine Learning**, a **parameter** refers to a configuration variable that is **internal to the model** and is learned from the training data.\n",
    "\n",
    "### âœ… In simple terms:\n",
    "\n",
    "A **parameter** is a value that the model **adjusts automatically** during training to **fit the data**.\n",
    "\n",
    "### ğŸ”§ Example:\n",
    "\n",
    "In **Linear Regression**, the equation is:\n",
    "\n",
    "$$\n",
    "y = w \\cdot x + b\n",
    "$$\n",
    "\n",
    "* `w` (weight) and `b` (bias) are **parameters**.\n",
    "* These values are **learned** during training using techniques like **Gradient Descent**.\n",
    "\n",
    "### ğŸ’¡ Key Points:\n",
    "\n",
    "* Parameters are learned **by the algorithm**.\n",
    "* They **define** the skill of the model on your data.\n",
    "* Common in models like:\n",
    "\n",
    "  * Linear Regression â†’ weights and bias\n",
    "  * Neural Networks â†’ weights of connections between layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f56471",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. What is correlation? What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca48c67",
   "metadata": {},
   "source": [
    "**Correlation** is a statistical measure that describes the **strength and direction of a relationship between two variables**.\n",
    "\n",
    "* It tells us **how much one variable changes when another variable changes**.\n",
    "* It ranges from **-1 to +1**.\n",
    "\n",
    "$$\n",
    "\\text{Correlation coefficient (r)} \\in [-1, 1]\n",
    "$$\n",
    "\n",
    "### ğŸ“ˆ Types of Correlation:\n",
    "\n",
    "| Correlation Type | Value of `r` | Meaning                                              |\n",
    "| ---------------- | ------------ | ---------------------------------------------------- |\n",
    "| **Positive**     | `0 < r â‰¤ 1`  | As one variable increases, the other also increases. |\n",
    "| **Negative**     | `-1 â‰¤ r < 0` | As one variable increases, the other decreases.      |\n",
    "| **Zero**         | `r = 0`      | No linear relationship between variables.            |\n",
    "\n",
    "### What is Negative Correlation?\n",
    "\n",
    "**Negative correlation** means that:\n",
    "\n",
    "* When one variable **increases**, the other **decreases**.\n",
    "* And vice versa.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "* Time spent **watching TV** vs. **grades** in school:\n",
    "  âŸ¶ As TV time goes **up**, grades might go **down** â†’ **Negative correlation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8d0fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9f4cb",
   "metadata": {},
   "source": [
    "**Machine Learning (ML)** is a field of computer science that enables systems to **learn from data** and **make decisions or predictions** without being explicitly programmed.\n",
    "\n",
    "> ğŸ“Œ In simple words:\n",
    "> ML is about teaching machines to **learn patterns from past data** and use them to **make decisions on new data**.\n",
    "\n",
    "### Main Components in Machine Learning:\n",
    "\n",
    "1. **Data**\n",
    "\n",
    "   * The foundation of any ML model.\n",
    "   * Can be labeled (for supervised learning) or unlabeled (for unsupervised learning).\n",
    "\n",
    "2. **Model**\n",
    "\n",
    "   * A mathematical structure or algorithm that makes predictions or decisions.\n",
    "   * Example: Linear Regression, Decision Trees, Neural Networks.\n",
    "\n",
    "3. **Features**\n",
    "\n",
    "   * The **input variables** used to make predictions.\n",
    "   * Example: Age, salary, height, etc.\n",
    "\n",
    "4. **Labels / Target**\n",
    "\n",
    "   * The **output** we want to predict (in supervised learning).\n",
    "   * Example: Predicting house price â†’ the price is the label.\n",
    "\n",
    "5. **Training**\n",
    "\n",
    "   * The process of feeding data to the model so it can **learn the patterns**.\n",
    "\n",
    "6. **Testing**\n",
    "\n",
    "   * Evaluating the modelâ€™s performance on **unseen data** to check how well it generalizes.\n",
    "\n",
    "7. **Algorithm**\n",
    "\n",
    "   * The method used to find patterns in data and update the model.\n",
    "   * Examples: Gradient Descent, K-Means, Backpropagation.\n",
    "\n",
    "8. **Loss Function**\n",
    "\n",
    "   * Measures the **error** between predicted output and actual label.\n",
    "   * Lower loss = better performance.\n",
    "\n",
    "9. **Optimizer**\n",
    "\n",
    "   * Algorithm that adjusts model parameters to minimize the loss.\n",
    "   * Example: SGD, Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d34400",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd750d48",
   "metadata": {},
   "source": [
    "The **loss value** is a **numerical measure** of how far the model's predictions are from the actual values (ground truth).\n",
    "It is calculated using a **loss function** like MSE (Mean Squared Error), Cross-Entropy, etc.\n",
    "\n",
    "### Why is it important?\n",
    "\n",
    "* **Low Loss** â†’ Model is making accurate predictions. âœ…\n",
    "* **High Loss** â†’ Model is making poor predictions. âŒ\n",
    "\n",
    "The **goal of training** a machine learning model is to **minimize this loss**.\n",
    "\n",
    "### Interpreting the Loss:\n",
    "\n",
    "| Loss Value | Meaning                          |\n",
    "| ---------- | -------------------------------- |\n",
    "| `0`        | Perfect predictions (ideal case) |\n",
    "| `Small`    | Good predictions (acceptable)    |\n",
    "| `Large`    | Model is not learning well       |\n",
    "\n",
    "### During training:\n",
    "\n",
    "* The **loss starts high**.\n",
    "* As training continues, the loss should **decrease steadily**.\n",
    "* If the loss **stops decreasing**, the model might:\n",
    "\n",
    "  * Be underfitting or overfitting,\n",
    "  * Need better features or hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b2eea",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38ee6e",
   "metadata": {},
   "source": [
    "### 1. **Continuous Variables** ğŸ“ˆ\n",
    "\n",
    "* Can take **any numerical value** within a range (including decimals).\n",
    "* Represent **measurable quantities**.\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "* Height (in cm)\n",
    "* Weight (in kg)\n",
    "* Temperature\n",
    "* Price of a house\n",
    "\n",
    "#### Key properties:\n",
    "\n",
    "* Infinite possible values.\n",
    "* Can be used in **regression problems**.\n",
    "\n",
    "### 2. **Categorical Variables** ğŸ·ï¸\n",
    "\n",
    "* Represent **discrete groups or categories**.\n",
    "* Cannot be measured numerically (though we may encode them as numbers).\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "* Gender: Male, Female, Other\n",
    "* Car Brand: Toyota, Ford, BMW\n",
    "* Color: Red, Blue, Green\n",
    "* Yes/No\n",
    "\n",
    "#### Key properties:\n",
    "\n",
    "* Finite number of distinct values.\n",
    "* Used in **classification problems**.\n",
    "\n",
    "###  **ğŸ§ Tip**:\n",
    "\n",
    "When using ML models:\n",
    "\n",
    "* Categorical variables are often **encoded** using techniques like **One-Hot Encoding** or **Label Encoding**.\n",
    "* Continuous variables are often **normalized** or **scaled**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e4a70",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d151f3",
   "metadata": {},
   "source": [
    "### 1. **Label Encoding** ğŸ”¢\n",
    "\n",
    "* Converts each category into a **unique number**.\n",
    "\n",
    "##### **Example**:\n",
    "\n",
    "| Color | Encoded |\n",
    "| ----- | ------- |\n",
    "| Red   | 0       |\n",
    "| Green | 1       |\n",
    "| Blue  | 2       |\n",
    "\n",
    "##### **Use When**:\n",
    "\n",
    "* Categories have an **ordinal relationship** (e.g., Low, Medium, High).\n",
    "\n",
    "##### âš ï¸ **Caution**:\n",
    "\n",
    "* Not suitable for **non-ordinal** data in tree-based or linear models â†’ may mislead the algorithm.\n",
    "\n",
    "### 2. **One-Hot Encoding** ğŸ¯\n",
    "\n",
    "* Creates **binary columns** for each category.\n",
    "\n",
    "##### **Example**:\n",
    "\n",
    "| Color | Red | Green | Blue |\n",
    "| ----- | --- | ----- | ---- |\n",
    "| Red   | 1   | 0     | 0    |\n",
    "| Green | 0   | 1     | 0    |\n",
    "| Blue  | 0   | 0     | 1    |\n",
    "\n",
    "##### **Use When**:\n",
    "\n",
    "* Categories are **nominal (no order)**.\n",
    "* Common in linear/logistic regression, neural networks.\n",
    "\n",
    "### 3. **Ordinal Encoding** ğŸ”¼\n",
    "\n",
    "* Assigns values based on **order** or **rank** of categories.\n",
    "\n",
    "##### **Example**:\n",
    "\n",
    "| Size   | Encoded |\n",
    "| ------ | ------- |\n",
    "| Small  | 0       |\n",
    "| Medium | 1       |\n",
    "| Large  | 2       |\n",
    "\n",
    "##### **Use When**:\n",
    "\n",
    "* The categories have **logical order**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6c0fd",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7ea31",
   "metadata": {},
   "source": [
    "In Machine Learning, we **split the dataset** into two main parts:\n",
    "\n",
    "### 1. **Training Dataset** ğŸ§ \n",
    "\n",
    "* This is the data used to **train the model** â€” i.e., to help it **learn patterns** and relationships between features and labels.\n",
    "* The model **adjusts its parameters** based on this data using algorithms like Gradient Descent.\n",
    "\n",
    "#### **Example**:\n",
    "\n",
    "If you're building a house price predictor, the training data teaches the model how factors like location and size affect price.\n",
    "\n",
    "### 2. **Testing Dataset** ğŸ§ª\n",
    "\n",
    "* This is **separate data** (unseen by the model during training) used to **evaluate how well the model performs**.\n",
    "* Helps check if the model can **generalize** to new, real-world data.\n",
    "\n",
    "#### **Example**:\n",
    "\n",
    "If your model predicts house prices well on the test set, it likely performs well in real scenarios.\n",
    "\n",
    "### ğŸ” **Typical Split**:\n",
    "\n",
    "* **80%** for training\n",
    "* **20%** for testing\n",
    "  (or sometimes 70â€“30, 60â€“40 depending on dataset size)\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### ğŸ¯ Why is this important?\n",
    "\n",
    "* Without testing, we **canâ€™t trust** the modelâ€™s performance.\n",
    "* Helps **detect overfitting** (model does well on training but poorly on new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455617c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd8751",
   "metadata": {},
   "source": [
    "`sklearn.preprocessing` is a **module in Scikit-learn** that provides tools to **prepare or transform data** before training a machine learning model.\n",
    "\n",
    "### ğŸ“¦ Why is it important?\n",
    "\n",
    "Raw data often:\n",
    "\n",
    "* Has different **scales** (e.g., age in years, salary in lakhs),\n",
    "* Contains **categorical values**,\n",
    "* Needs to be **normalized, standardized**, or **encoded**.\n",
    "\n",
    "The `preprocessing` module helps make data **clean, consistent, and ML-friendly**.\n",
    "\n",
    "### ğŸ§° Common Tools in `sklearn.preprocessing`:\n",
    "\n",
    "| Function / Class       | What It Does                                    | Example Use                       |\n",
    "| ---------------------- | ----------------------------------------------- | --------------------------------- |\n",
    "| `StandardScaler()`     | Standardizes features (mean = 0, std = 1)       | Good for algorithms like SVM, KNN |\n",
    "| `MinMaxScaler()`       | Scales data to a given range (default 0 to 1)   | Useful for neural networks        |\n",
    "| `LabelEncoder()`       | Converts categorical labels to numeric codes    | For target variables              |\n",
    "| `OneHotEncoder()`      | Converts categorical features to one-hot format | For input features                |\n",
    "| `Binarizer()`          | Converts values above a threshold to 1, else 0  | For binary classification         |\n",
    "| `PolynomialFeatures()` | Generates polynomial and interaction terms      | For polynomial regression         |\n",
    "\n",
    "### ğŸ§  Example:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [[1.0], [2.0], [3.0]]\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10385b5",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b0aa8",
   "metadata": {},
   "source": [
    "A **test set** is a **portion of the dataset** that is **kept aside** and **not used during model training**.\n",
    "It is used to **evaluate** how well a trained model performs on **new, unseen data**.\n",
    "\n",
    "### ğŸ¯ Purpose of the Test Set:\n",
    "\n",
    "* To **measure the model's generalization ability**.\n",
    "* To detect problems like **overfitting** (doing well on training but poorly on unseen data).\n",
    "* To provide an **honest estimate** of the modelâ€™s real-world performance.\n",
    "\n",
    "### ğŸ“Š Typical Dataset Split:\n",
    "\n",
    "| Dataset Part | Use              | Typical Size |\n",
    "| ------------ | ---------------- | ------------ |\n",
    "| Training Set | Model learning   | 70â€“80%       |\n",
    "| Test Set     | Final evaluation | 20â€“30%       |\n",
    "\n",
    "Sometimes a **validation set** is also used during tuning.\n",
    "\n",
    "### ğŸ§  Python Example:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### ğŸ” Summary:\n",
    "\n",
    "| Feature        | Test Set                      |\n",
    "| -------------- | ----------------------------- |\n",
    "| Seen by model? | âŒ No                        |\n",
    "| When used?     | After training is complete    |\n",
    "| Purpose        | Evaluate final model accuracy |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533e263",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd880e76",
   "metadata": {},
   "source": [
    "### âœ… **Part 1: How to Split Data for Training and Testing in Python**\n",
    "\n",
    "We use **`train_test_split`** from `sklearn.model_selection` to split the dataset into training and testing sets.\n",
    "\n",
    "##### ğŸ“Œ **Syntax**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "##### ğŸ“‹ **Explanation**:\n",
    "\n",
    "* `X`: Features (input variables)\n",
    "* `y`: Labels (output variable)\n",
    "* `test_size=0.2`: 20% data goes to test set, 80% to train set\n",
    "* `random_state=42`: Ensures reproducibility (same split every time)\n",
    "\n",
    "### âœ… **Part 2: How Do You Approach a Machine Learning Problem?**\n",
    "\n",
    "Hereâ€™s a **step-by-step approach**:\n",
    "\n",
    "#### 1. **Understand the Problem**\n",
    "\n",
    "* What are you trying to predict or classify?\n",
    "* Is it a **regression**, **classification**, or **clustering** problem?\n",
    "\n",
    "#### 2. **Collect and Explore Data**\n",
    "\n",
    "* Load the dataset using `pandas`, `numpy`, etc.\n",
    "* Use `.head()`, `.info()`, `.describe()` to explore it.\n",
    "\n",
    "#### 3. **Preprocess the Data**\n",
    "\n",
    "* Handle missing values\n",
    "* Encode categorical variables (e.g., LabelEncoder, OneHotEncoder)\n",
    "* Scale/normalize numerical features\n",
    "* Feature selection/engineering\n",
    "\n",
    "#### 4. **Split the Dataset**\n",
    "\n",
    "* Use `train_test_split()` to divide into training and testing sets\n",
    "\n",
    "#### 5. **Choose a Model**\n",
    "\n",
    "* For classification: Logistic Regression, Decision Tree, SVM, etc.\n",
    "* For regression: Linear Regression, Random Forest Regressor, etc.\n",
    "\n",
    "#### 6. **Train the Model**\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### 7. **Evaluate the Model**\n",
    "\n",
    "* Use metrics like:\n",
    "\n",
    "  * **Accuracy**, **Precision**, **Recall**, **F1-score** (classification)\n",
    "  * **MAE**, **MSE**, **RMSE**, **RÂ² score** (regression)\n",
    "\n",
    "#### 8. **Tune Hyperparameters**\n",
    "\n",
    "* Use `GridSearchCV`, `RandomizedSearchCV`, or manual tuning\n",
    "\n",
    "#### 9. **Test on Unseen Data**\n",
    "\n",
    "* Check performance on `X_test`, `y_test`\n",
    "\n",
    "#### 10. **Deploy the Model (Optional)**\n",
    "\n",
    "* Export using `joblib` or `pickle`\n",
    "* Deploy via Flask/Django or cloud platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d307eba",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07899e00",
   "metadata": {},
   "source": [
    "**EDA** is a crucial step in any Machine Learning pipeline.\n",
    "It helps you **understand your data** before using it to train a model.\n",
    "\n",
    "### ğŸ¯ **Main Reasons for Performing EDA:**\n",
    "\n",
    "#### 1. **Understand the Data Structure** ğŸ“Š\n",
    "\n",
    "* What are the features (columns)?\n",
    "* What is the target (label)?\n",
    "* What data types are present?\n",
    "\n",
    "ğŸ”¹ Example: Is \"Age\" stored as an integer or string?\n",
    "\n",
    "#### 2. **Detect Missing or Invalid Data** âš ï¸\n",
    "\n",
    "* EDA helps you find **null values**, **outliers**, or **inconsistent entries**.\n",
    "\n",
    "ğŸ”¹ Example: \"Salary\" has some missing values or negative numbers.\n",
    "\n",
    "#### 3. **Identify Feature Distributions** ğŸ“ˆ\n",
    "\n",
    "* See how values are spread (normal distribution, skewed, etc.)\n",
    "* Helps decide if **scaling or transformation** is needed.\n",
    "\n",
    "#### 4. **Reveal Patterns and Relationships** ğŸ”\n",
    "\n",
    "* Use **correlation heatmaps** and **scatter plots** to find relationships between variables.\n",
    "* Helps in **feature selection**.\n",
    "\n",
    "ğŸ”¹ Example: \"Experience\" and \"Salary\" are strongly correlated.\n",
    "\n",
    "#### 5. **Detect Outliers** ğŸš¨\n",
    "\n",
    "* Outliers can distort model training and metrics.\n",
    "* EDA helps visualize and handle them using boxplots or z-scores.\n",
    "\n",
    "#### 6. **Understand Class Imbalance** âš–ï¸\n",
    "\n",
    "* In classification problems, check if one class dominates.\n",
    "* Helps decide whether to use **resampling**, **class weights**, etc.\n",
    "\n",
    "ğŸ”¹ Example: 90% of your data belongs to class A â€” this needs fixing.\n",
    "\n",
    "#### 7. **Build Intuition** ğŸ¤”\n",
    "\n",
    "* Before letting an algorithm make decisions, **you** should understand whatâ€™s going on in the data.\n",
    "\n",
    "### ğŸ“Š Common EDA Tools in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "sns.pairplot(df)\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d849bdfb",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22257406",
   "metadata": {},
   "source": [
    "**Correlation** is a **statistical measure** that describes the **strength and direction of a relationship** between two variables.\n",
    "\n",
    "It tells us:\n",
    "\n",
    "> â *How does one variable change when another variable changes?* â\n",
    "\n",
    "### ğŸ“ Correlation Coefficient (r):\n",
    "\n",
    "* Ranges between **-1 and +1**\n",
    "\n",
    "| Value of `r` | Meaning                                                           |\n",
    "| ------------ | ----------------------------------------------------------------- |\n",
    "| **+1**       | Perfect positive correlation (both increase together)             |\n",
    "| **0**        | No correlation                                                    |\n",
    "| **â€“1**       | Perfect negative correlation (one increases, the other decreases) |\n",
    "\n",
    "### ğŸ”¹ Types of Correlation:\n",
    "\n",
    "| Type of Correlation | Behavior                                       |\n",
    "| ------------------- | ---------------------------------------------- |\n",
    "| **Positive**        | As one variable increases, the other increases |\n",
    "| **Negative**        | As one variable increases, the other decreases |\n",
    "| **Zero**            | No linear relationship between variables       |\n",
    "\n",
    "### ğŸ“Š Example:\n",
    "\n",
    "| Hours Studied | Marks Scored |\n",
    "| ------------- | ------------ |\n",
    "| 1             | 50           |\n",
    "| 2             | 60           |\n",
    "| 3             | 70           |\n",
    "\n",
    "This shows a **positive correlation** â€” more hours studied â†’ higher marks.\n",
    "\n",
    "### ğŸ§  Python Code Example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Hours': [1, 2, 3, 4],\n",
    "        'Marks': [50, 60, 70, 80]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.corr())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a8aa4a",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adcb9d",
   "metadata": {},
   "source": [
    "**Negative correlation** means that as **one variable increases**, the **other variable decreases** â€” and vice versa.\n",
    "\n",
    "### ğŸ“ Correlation Coefficient (r):\n",
    "\n",
    "* A **negative correlation** has a value of:\n",
    "\n",
    "  $$\n",
    "  -1 < r < 0\n",
    "  $$\n",
    "\n",
    "* The closer it is to **-1**, the **stronger** the negative relationship.\n",
    "\n",
    "### ğŸ“Š Example:\n",
    "\n",
    "| Hours of TV Watched | Exam Score |\n",
    "| ------------------- | ---------- |\n",
    "| 1                   | 95         |\n",
    "| 2                   | 85         |\n",
    "| 3                   | 75         |\n",
    "| 4                   | 65         |\n",
    "\n",
    "* As **TV time increases**, **exam scores decrease**\n",
    "  â†’ This is a **negative correlation**.\n",
    "\n",
    "### ğŸ“‰ Visual Understanding:\n",
    "\n",
    "In a scatter plot, a negative correlation will show a **downward trend** from left to right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecae08",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde32cd",
   "metadata": {},
   "source": [
    "You can find the correlation using **`pandas`** or **`numpy`**, and visualize it using **`seaborn`** or **`matplotlib`**.\n",
    "\n",
    "### **Using `pandas.corr()`**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
    "    'Marks': [55, 60, 65, 70, 75],\n",
    "    'TV_Hours': [5, 4, 3, 2, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "print(df.corr())\n",
    "```\n",
    "\n",
    "#### ğŸ” Output:\n",
    "\n",
    "```raw\n",
    "              Hours_Studied     Marks   TV_Hours\n",
    "Hours_Studied        1.000     1.000     -1.000\n",
    "Marks                1.000     1.000     -1.000\n",
    "TV_Hours            -1.000    -1.000      1.000\n",
    "```\n",
    "\n",
    "### ğŸ“Š Visualizing with a Heatmap\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### ğŸ§  Notes:\n",
    "\n",
    "* `.corr()` uses **Pearson correlation** by default.\n",
    "* For other types like **Spearman** or **Kendall**, use:\n",
    "\n",
    "  ```python\n",
    "  df.corr(method='spearman')\n",
    "  df.corr(method='kendall')\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714a92b",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341741f",
   "metadata": {},
   "source": [
    "**Causation** means that **one variable directly affects or causes a change in another variable**.\n",
    "\n",
    "> ğŸ“Œ In other words:\n",
    "> If **A causes B**, then **changing A will directly change B**.\n",
    "\n",
    "### âš–ï¸ Difference Between Correlation and Causation:\n",
    "\n",
    "| Aspect       | Correlation                   | Causation                                          |\n",
    "| ------------ | ----------------------------- | -------------------------------------------------- |\n",
    "| Meaning      | Two variables move together   | One variable **directly causes** change in another |\n",
    "| Direction    | May or may not be directional | Always directional (cause â†’ effect)                |\n",
    "| Guarantee    | Does **not** imply causation  | Implies some degree of control or influence        |\n",
    "| Example Tool | `.corr()` in Python           | Requires **experiments or domain knowledge**       |\n",
    "\n",
    "### ğŸ“Š Example:\n",
    "\n",
    "#### ğŸ”¹ Correlation:\n",
    "\n",
    "* **Ice cream sales** and **drowning cases** are positively correlated.\n",
    "* Does ice cream cause drowning? âŒ No.\n",
    "\n",
    "#### ğŸ”¹ Causation:\n",
    "\n",
    "* **Smoking** causes **lung cancer**.\n",
    "* This is backed by **medical studies** and **experiments** â†’ âœ… **Causation**.\n",
    "\n",
    "### ğŸš« Common Mistake:\n",
    "\n",
    "> â€œCorrelation â‰  Causationâ€\n",
    "\n",
    "Just because two variables move together doesnâ€™t mean one causes the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f883c75",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ad59c",
   "metadata": {},
   "source": [
    "An **optimizer** is an algorithm that **adjusts the parameters (like weights and biases)** of a machine learning model to **minimize the loss function** during training.\n",
    "\n",
    "> ğŸ“Œ Think of it as the â€œbrainâ€ behind how the model **learns** from mistakes and gets better.\n",
    "\n",
    "### ğŸ¯ Why is an Optimizer Needed?\n",
    "\n",
    "* During training, the model predicts, compares with the true value (using a **loss function**), and updates itself.\n",
    "* The optimizer decides **how to update the weights** so the loss gets smaller.\n",
    "\n",
    "### âš™ï¸ Common Optimizers in Deep Learning:\n",
    "\n",
    "| Optimizer                             | Description                                                   |\n",
    "| ------------------------------------- | ------------------------------------------------------------- |\n",
    "| **Gradient Descent (GD)**             | Basic optimizer, updates weights in direction of lowest loss. |\n",
    "| **Stochastic Gradient Descent (SGD)** | Updates weights after each training sample.                   |\n",
    "| **Mini-batch Gradient Descent**       | Combines GD and SGD â€“ updates in small batches.               |\n",
    "| **Momentum**                          | Speeds up SGD by considering previous gradients.              |\n",
    "| **AdaGrad**                           | Adapts learning rate individually for each parameter.         |\n",
    "| **RMSprop**                           | Like AdaGrad but avoids rapidly decreasing learning rates.    |\n",
    "| **Adam**                              | Combines Momentum + RMSprop (most widely used).               |\n",
    "\n",
    "### 1. **Gradient Descent (Batch GD)**\n",
    "\n",
    "* Computes gradient over the **entire dataset**.\n",
    "\n",
    "```python\n",
    "# Used mostly for teaching, not scalable for large data\n",
    "```\n",
    "\n",
    "### 2. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "* Updates weights for **each data point**.\n",
    "* Faster, but **noisy** updates.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.01), loss='mse')\n",
    "```\n",
    "\n",
    "### 3. **Momentum**\n",
    "\n",
    "* Adds **\"inertia\"** to gradient descent.\n",
    "* Helps avoid getting stuck in local minima.\n",
    "\n",
    "```python\n",
    "SGD(learning_rate=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "### 4. **AdaGrad**\n",
    "\n",
    "* Adjusts learning rate **based on past gradients**.\n",
    "* Good for sparse data (e.g., NLP).\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "model.compile(optimizer=Adagrad(learning_rate=0.01), loss='mse')\n",
    "```\n",
    "\n",
    "### 5. **RMSprop**\n",
    "\n",
    "* Solves AdaGradâ€™s issue of decaying learning rate.\n",
    "* Good for **recurrent neural networks**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.001), loss='mse')\n",
    "```\n",
    "\n",
    "### 6. **Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "* Combines **Momentum + RMSprop**.\n",
    "* Most widely used in practice for deep learning.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739a5f4",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. What is sklearn.linear_model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a7c30",
   "metadata": {},
   "source": [
    "`sklearn.linear_model` is a **module in Scikit-learn** that contains a collection of **linear models** used for **regression and classification tasks**.\n",
    "\n",
    "### ğŸ“¦ What It Offers:\n",
    "\n",
    "This module provides implementations for:\n",
    "\n",
    "* **Linear Regression**\n",
    "* **Logistic Regression**\n",
    "* **Ridge**, **Lasso**, **ElasticNet**\n",
    "* **SGDClassifier**, **Perceptron**, etc.\n",
    "\n",
    "These models are called **linear** because they assume a **linear relationship** between inputs and outputs.\n",
    "\n",
    "### ğŸ§  Common Models in `sklearn.linear_model`:\n",
    "\n",
    "| Model                | Purpose        | Description                                                       |\n",
    "| -------------------- | -------------- | ----------------------------------------------------------------- |\n",
    "| `LinearRegression`   | Regression     | Predicts continuous values                                        |\n",
    "| `LogisticRegression` | Classification | Predicts class labels (binary/multiclass)                         |\n",
    "| `Ridge`              | Regression     | Linear regression with **L2 regularization**                      |\n",
    "| `Lasso`              | Regression     | Linear regression with **L1 regularization**                      |\n",
    "| `ElasticNet`         | Regression     | Combines **L1 and L2 regularization**                             |\n",
    "| `SGDClassifier`      | Classification | Linear classifier optimized using **Stochastic Gradient Descent** |\n",
    "\n",
    "### Example 1: Linear Regression\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### Example 2: Logistic Regression\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "### When to Use It?\n",
    "\n",
    "* When your data shows a **linear trend**.\n",
    "* For **interpretable models**.\n",
    "* When you want to add **regularization** (Ridge/Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80adace",
   "metadata": {},
   "source": [
    "---\n",
    "## 18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136728d6",
   "metadata": {},
   "source": [
    "The `model.fit()` method is used to **train** a machine learning model on your **training data**.\n",
    "\n",
    "> ğŸ“Œ It tells the model:\n",
    "> \"**Here is the data â€” learn the patterns from it.**\"\n",
    "\n",
    "### ğŸ” What Happens Inside?\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "The model:\n",
    "\n",
    "1. Takes the **input features** (`X_train`)\n",
    "2. Takes the **target labels** (`y_train`)\n",
    "3. Applies the learning algorithm (e.g., gradient descent)\n",
    "4. **Learns** the best values for internal parameters (like weights)\n",
    "\n",
    "### ğŸ§¾ Required Arguments:\n",
    "\n",
    "| Argument  | Description                                         |\n",
    "| --------- | --------------------------------------------------- |\n",
    "| `X_train` | Feature matrix (input data) â€” 2D array or DataFrame |\n",
    "| `y_train` | Target values (labels) â€” 1D array or Series         |\n",
    "\n",
    "### ğŸ§  Example:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### ğŸ§° Optional Arguments (for some models):\n",
    "\n",
    "* `sample_weight`: Weights for each instance\n",
    "* `epochs`, `batch_size`: For deep learning models (e.g., in TensorFlow/Keras)\n",
    "* `validation_data`: To monitor validation loss during training (in neural networks)\n",
    "\n",
    "### ğŸ“Š After `.fit()` you can:\n",
    "\n",
    "* Use `.predict(X_test)` to make predictions\n",
    "* Use `.score(X_test, y_test)` to get model accuracy/performance\n",
    "* Access learned parameters (e.g., `.coef_`, `.intercept_` in linear models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a6464",
   "metadata": {},
   "source": [
    "---\n",
    "## 19. What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bbe9d8",
   "metadata": {},
   "source": [
    "The `model.predict()` method is used to make **predictions** on **new/unseen input data** using a model that has already been **trained** with `.fit()`.\n",
    "\n",
    "> ğŸ“Œ In simple terms:\n",
    "> \"**Take the trained model and use it to guess the output for new inputs.**\"\n",
    "\n",
    "### ğŸ” How it works:\n",
    "\n",
    "After training:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "You can use:\n",
    "\n",
    "```python\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "The model uses the patterns it learned to **predict output values** (e.g., class labels or numerical values).\n",
    "\n",
    "### ğŸ§¾ Required Argument:\n",
    "\n",
    "| Argument | Description                                                                                                      |\n",
    "| -------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| `X_test` | Input data (features only) you want predictions for. Must be the **same shape and format** as the training data. |\n",
    "\n",
    "### ğŸ§  Example:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "* For regression â†’ `y_pred` contains **continuous values**\n",
    "* For classification â†’ `y_pred` contains **class labels**\n",
    "\n",
    "### âš ï¸ Notes:\n",
    "\n",
    "* Do **not** pass target values (`y_test`) to `predict()` â€” only features (`X_test`).\n",
    "* If input data shape or type doesnâ€™t match what the model expects, it will raise an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a8a455",
   "metadata": {},
   "source": [
    "---\n",
    "## 20. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b7565",
   "metadata": {},
   "source": [
    "In Machine Learning and Statistics, variables (also called **features**) are often classified into two main types:\n",
    "\n",
    "### 1. **Continuous Variables** ğŸ“ˆ\n",
    "\n",
    "* Can take **any numeric value** within a range (including decimals).\n",
    "* Represent **measurable quantities**.\n",
    "* Typically used in **regression problems**.\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "* Temperature (Â°C)\n",
    "* Height (cm)\n",
    "* Weight (kg)\n",
    "* Price (â‚¹)\n",
    "\n",
    "#### Key Properties:\n",
    "\n",
    "* Infinite possible values\n",
    "* Can be scaled or normalized\n",
    "\n",
    "### 2. **Categorical Variables** ğŸ·ï¸\n",
    "\n",
    "* Represent **discrete categories or groups**\n",
    "* Cannot be measured numerically (though they may be encoded as numbers)\n",
    "* Typically used in **classification problems**\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "* Gender: Male, Female\n",
    "* Color: Red, Green, Blue\n",
    "* Car Brand: Toyota, Ford, BMW\n",
    "* Yes/No, True/False\n",
    "\n",
    "#### Key Properties:\n",
    "\n",
    "* Finite possible values\n",
    "* Often need to be encoded (e.g., Label Encoding, One-Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03183644",
   "metadata": {},
   "source": [
    "---\n",
    "## 21. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae2fe1",
   "metadata": {},
   "source": [
    "**Feature scaling** is the process of **normalizing or standardizing** the range of independent variables (features) so that they contribute **equally** to the learning process.\n",
    "\n",
    "### ğŸ“Œ Why Feature Scaling Is Important:\n",
    "\n",
    "Many ML algorithms (like **KNN, SVM, Gradient Descent**) are sensitive to the **magnitude** of features. If one feature ranges from 0â€“1000 and another from 0â€“1, the model may become **biased** toward the larger-scale feature.\n",
    "\n",
    "### ğŸ§  Example:\n",
    "\n",
    "| Feature     | Before Scaling     |\n",
    "| ----------- | ------------------ |\n",
    "| Age (years) | 18, 25, 35, 60     |\n",
    "| Income (â‚¹)  | 15,000 to 2,00,000 |\n",
    "\n",
    "> Without scaling, \"Income\" dominates \"Age\".\n",
    "\n",
    "### âš™ï¸ Common Feature Scaling Methods:\n",
    "\n",
    "| Method              | Formula / Description                                  | Use Case                     |\n",
    "| ------------------- | ------------------------------------------------------ | ---------------------------- |\n",
    "| **Min-Max Scaling** | $x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$ â†’ \\[0, 1] | When you need bounded output |\n",
    "| **Standardization** | $x' = \\frac{x - \\mu}{\\sigma}$ â†’ mean = 0, std = 1      | Preferred for Gaussian data  |\n",
    "| **Robust Scaling**  | Scales using median & IQR (less sensitive to outliers) | Data with outliers           |\n",
    "\n",
    "### ğŸ“¦ In Python (with `sklearn.preprocessing`):\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### ğŸš€ Helps in ML By:\n",
    "\n",
    "* Improving **convergence speed** in gradient descent\n",
    "* Giving **equal importance** to all features\n",
    "* Avoiding **bias** due to scale differences\n",
    "* Making **distance-based models** (like KNN, SVM) perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f669d",
   "metadata": {},
   "source": [
    "---\n",
    "## 22. How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31e7f3",
   "metadata": {},
   "source": [
    "You can easily perform feature scaling using **`scikit-learn`'s** `preprocessing` module.\n",
    "\n",
    "### 1. **Standardization (Z-score Normalization)**\n",
    "\n",
    "Transforms data to have **mean = 0** and **standard deviation = 1**.\n",
    "\n",
    "#### ğŸ“¦ Using `StandardScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### 2. **Min-Max Scaling**\n",
    "\n",
    "Scales features to a **fixed range**, usually \\[0, 1].\n",
    "\n",
    "#### ğŸ“¦ Using `MinMaxScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### 3. **Robust Scaling**\n",
    "\n",
    "Scales features using **median and IQR**, robust to **outliers**.\n",
    "\n",
    "#### ğŸ“¦ Using `RobustScaler`:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### ğŸ§  Example with a Dataset:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    'Age': [18, 22, 25, 30, 35],\n",
    "    'Income': [15000, 18000, 25000, 40000, 60000]\n",
    "})\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "print(scaled_df)\n",
    "```\n",
    "\n",
    "### âš ï¸ Notes:\n",
    "\n",
    "* Always **fit on training data** and **transform on test data** to prevent data leakage.\n",
    "* For pipelines: use `Pipeline()` from `sklearn.pipeline` to apply scaling + modeling in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8350d39",
   "metadata": {},
   "source": [
    "---\n",
    "## 23. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4d6f6",
   "metadata": {},
   "source": [
    "`sklearn.preprocessing` is a **module in scikit-learn** that provides a set of **utility functions and classes** for **transforming raw data** into a format suitable for machine learning models.\n",
    "\n",
    "### ğŸ¯ Why Use It?\n",
    "\n",
    "Raw data may contain:\n",
    "\n",
    "* Different **scales** (age vs. income)\n",
    "* **Missing values**\n",
    "* **Categorical** variables\n",
    "* **Outliers**\n",
    "\n",
    "`sklearn.preprocessing` helps you **prepare and clean** this data effectively before model training.\n",
    "\n",
    "### ğŸ”§ Common Tools in `sklearn.preprocessing`:\n",
    "\n",
    "| Class/Function       | Purpose                                            |\n",
    "| -------------------- | -------------------------------------------------- |\n",
    "| `StandardScaler`     | Standardize features (mean = 0, std = 1)           |\n",
    "| `MinMaxScaler`       | Scale features to a given range (e.g. 0â€“1)         |\n",
    "| `RobustScaler`       | Scale using median and IQR (resistant to outliers) |\n",
    "| `LabelEncoder`       | Encode categorical labels as integers              |\n",
    "| `OneHotEncoder`      | Convert categorical features into one-hot vectors  |\n",
    "| `OrdinalEncoder`     | Encode categories with ordered numbers             |\n",
    "| `Binarizer`          | Convert data into binary (0/1) based on threshold  |\n",
    "| `PolynomialFeatures` | Generate polynomial and interaction terms          |\n",
    "| `Normalizer`         | Normalize input rows to unit norm                  |\n",
    "\n",
    "### ğŸ§  Example:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "### ğŸ“ Think of `sklearn.preprocessing` as:\n",
    "\n",
    "> ğŸ§¹ A \"data cleaning and transformation toolbox\" for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b34d33",
   "metadata": {},
   "source": [
    "---\n",
    "## 24. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fc242",
   "metadata": {},
   "source": [
    "In **Machine Learning**, we typically split the dataset into:\n",
    "\n",
    "* **Training set**: used to train the model\n",
    "* **Testing set**: used to evaluate model performance\n",
    "\n",
    "### ğŸ“¦ `train_test_split()` in scikit-learn\n",
    "\n",
    "The `train_test_split()` function from `sklearn.model_selection` is the most common way to do this.\n",
    "\n",
    "### ğŸ§  Syntax:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### ğŸ“Œ Parameters:\n",
    "\n",
    "| Parameter      | Description                                 |\n",
    "| -------------- | ------------------------------------------- |\n",
    "| `X`            | Features (independent variables)            |\n",
    "| `y`            | Target (dependent variable)                 |\n",
    "| `test_size`    | Proportion of test data (e.g., `0.2` = 20%) |\n",
    "| `random_state` | Sets seed to reproduce the same split       |\n",
    "\n",
    "### ğŸ§ª Example:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "print(\"Training shape:\", X_train.shape)\n",
    "print(\"Testing shape:\", X_test.shape)\n",
    "```\n",
    "\n",
    "### âœ… Tips:\n",
    "\n",
    "* Use `stratify=y` to maintain class balance in classification.\n",
    "* Always scale **after** splitting to avoid **data leakage**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c86357",
   "metadata": {},
   "source": [
    "---\n",
    "## 25. Explain data encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e6e6c",
   "metadata": {},
   "source": [
    "**Data encoding** is the process of **converting categorical data** (non-numeric) into **numerical format** so that machine learning models can process and learn from it.\n",
    "\n",
    "Most ML algorithms (like logistic regression, SVM, decision trees) work only with numbers, not text.\n",
    "\n",
    "### ğŸ¯ **Why is Encoding Needed**?\n",
    "\n",
    "Imagine a dataset:\n",
    "\n",
    "| Gender | Purchased |\n",
    "| ------ | --------- |\n",
    "| Male   | Yes       |\n",
    "| Female | No        |\n",
    "\n",
    "You canâ€™t feed \"Male\"/\"Female\" or \"Yes\"/\"No\" directly into a model â€” they must be **numerically encoded**.\n",
    "\n",
    "### ğŸ”§ **Types of Encoding**\n",
    "\n",
    "| Type                 | Use Case                                | Example                          |\n",
    "| -------------------- | --------------------------------------- | -------------------------------- |\n",
    "| **Label Encoding**   | Convert categories into integers        | Male â†’ 1, Female â†’ 0             |\n",
    "| **One-Hot Encoding** | Create binary columns for each category | Male â†’ \\[1, 0], Female â†’ \\[0, 1] |\n",
    "| **Ordinal Encoding** | Encode categories with a defined order  | Low â†’ 1, Medium â†’ 2, High â†’ 3    |\n",
    "\n",
    "### **Example (Label Encoding)**:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "gender_encoded = le.fit_transform(['Male', 'Female', 'Male'])  # Output: [1, 0, 1]\n",
    "```\n",
    "\n",
    "### **Example (One-Hot Encoding)**:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "gender = np.array([['Male'], ['Female'], ['Male']])\n",
    "encoded = encoder.fit_transform(gender)\n",
    "# Output: [[1., 0.], [0., 1.], [1., 0.]]\n",
    "```\n",
    "\n",
    "### ğŸ“Œ Note:\n",
    "\n",
    "* **Label Encoding** is simpler, but may confuse the model into thinking there's an order between categories.\n",
    "* **One-Hot Encoding** avoids this by using separate columns for each category."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
